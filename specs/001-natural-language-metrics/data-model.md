# Phase 1 Design: Data Model

**Feature**: Natural Language Metrics Querying  
**Status**: Complete  
**Based on**: [specification](spec.md) and [research](research.md)

---

## Overview

The data model for the natural language metrics system consists of three primary entities:
1. **MetricsQuery**: Parsed query parameters from natural language
2. **MetricsQueryResult**: Time-series data returned from Grafana
3. **QueryError**: Error information for failed queries

All models use Pydantic for validation and automatic JSON schema generation.

---

## Entity: MetricsQuery

**Purpose**: Represents a user's natural language query parsed into structured parameters.

**Responsibility**: Provide validated parameters for Grafana API calls.

**Persistence**: Stateless - not persisted, reconstructed per query.

### Fields

```python
class TimeRange(BaseModel):
    """Time range specification for queries."""
    start_time: datetime
    end_time: datetime
    
    @validator('end_time')
    def end_after_start(cls, v, values):
        if 'start_time' in values and v <= values['start_time']:
            raise ValueError("end_time must be after start_time")
        return v

class MetricsQuery(BaseModel):
    """
    Represents a natural language metric query parsed into structured parameters.
    Generated by LLM from user's natural language input.
    """
    
    # Primary identifiers
    metric_name: str = Field(
        description="Name of the metric to query (e.g., 'cpu_usage', 'memory_utilization')"
    )
    
    # Time range
    time_range: TimeRange = Field(
        description="Query time range (start_time, end_time)"
    )
    
    # Optional parameters
    aggregation: Optional[str] = Field(
        default=None,
        description="Optional aggregation function: 'avg', 'max', 'min', 'sum'"
    )
    
    filters: Optional[Dict[str, str]] = Field(
        default=None,
        description="Optional filters (e.g., {'instance': 'server-1', 'region': 'us-east'})"
    )
    
    # Metadata
    confidence: float = Field(
        default=1.0,
        description="LLM confidence in this interpretation (0.0-1.0)"
    )
    
    @validator('aggregation')
    def validate_aggregation(cls, v):
        """Ensure aggregation is one of allowed values."""
        if v and v not in ['avg', 'max', 'min', 'sum']:
            raise ValueError(f"Invalid aggregation: {v}. Must be one of: avg, max, min, sum")
        return v

class Config:
    json_schema_extra = {
        "examples": [
            {
                "metric_name": "cpu_usage",
                "time_range": {
                    "start_time": "2026-01-21T10:00:00Z",
                    "end_time": "2026-01-21T11:00:00Z"
                }
            },
            {
                "metric_name": "memory_utilization",
                "time_range": {
                    "start_time": "2026-01-20T00:00:00Z",
                    "end_time": "2026-01-21T00:00:00Z"
                },
                "aggregation": "avg",
                "filters": {"instance": "prod-server-1"}
            }
        ]
    }
```

### Validation Rules

| Field | Rule | Rationale |
|-------|------|-----------|
| `metric_name` | Non-empty string | Must identify a metric |
| `start_time` | ISO 8601 datetime | Consistent time format |
| `end_time` | After `start_time` | Logical time range |
| `aggregation` | One of allowed values | Prevents invalid Grafana queries |
| `confidence` | 0.0-1.0 | Indicates LLM certainty |

### State Transitions

```
Natural Language Input
        │
        ▼
LLM Parsing (ChatGPT/Ollama)
        │
        ▼
MetricsQuery (instantiated with validation)
        ├─ Valid → Proceed to Grafana query
        └─ Invalid → Error response to user
```

---

## Entity: MetricsQueryResult

**Purpose**: Represents the time-series data returned from Grafana for a metric.

**Responsibility**: Provide validated, formatted metric data for display to user.

**Persistence**: Stateless - calculated per query, not stored.

### Fields

```python
class DataPoint(BaseModel):
    """Single metric data point."""
    value: float
    timestamp: datetime
    
    @validator('value')
    def value_is_finite(cls, v):
        """Ensure value is finite (not NaN or Inf)."""
        if not (-float('inf') < v < float('inf')):
            raise ValueError(f"Data point value must be finite, got {v}")
        return v

class AggregationStats(BaseModel):
    """Optional aggregation statistics."""
    min: float
    max: float
    mean: float
    median: float
    sum: float
    count: int

class MetricsQueryResult(BaseModel):
    """
    Represents time-series metric data from Grafana.
    Validated and formatted for human consumption.
    """
    
    # Metadata
    metric_name: str = Field(
        description="Name of the queried metric"
    )
    
    unit: str = Field(
        description="Unit of measurement (e.g., '%', 'ms', 'bytes')"
    )
    
    time_range: TimeRange = Field(
        description="Actual time range of returned data"
    )
    
    # Data
    datapoints: List[DataPoint] = Field(
        description="List of (timestamp, value) pairs"
    )
    
    # Optional calculations
    aggregation_applied: Optional[str] = Field(
        default=None,
        description="Which aggregation was applied (if any)"
    )
    
    statistics: Optional[AggregationStats] = Field(
        default=None,
        description="Calculated statistics over the data points"
    )
    
    # Response quality
    datapoint_count: int = Field(
        description="Number of data points returned"
    )
    
    is_empty: bool = Field(
        default=False,
        description="True if no data available for this query"
    )
    
    @property
    def summary(self) -> str:
        """Generate human-readable summary of results."""
        if self.is_empty:
            return f"No data available for {self.metric_name} in the specified time range."
        
        stats = self.statistics or self._calculate_stats()
        return (
            f"{self.metric_name}: {self.datapoint_count} data points\n"
            f"  Range: {stats.min:.2f} to {stats.max:.2f} {self.unit}\n"
            f"  Average: {stats.mean:.2f} {self.unit}\n"
            f"  Time: {self.time_range.start_time} to {self.time_range.end_time}"
        )
    
    def _calculate_stats(self) -> AggregationStats:
        """Calculate statistics from data points."""
        if not self.datapoints:
            raise ValueError("Cannot calculate stats for empty data")
        
        values = [dp.value for dp in self.datapoints]
        return AggregationStats(
            min=min(values),
            max=max(values),
            mean=sum(values) / len(values),
            median=self._median(values),
            sum=sum(values),
            count=len(values)
        )
```

### Format for Human Display

```
CPU Usage
  Time Period: 2026-01-21 10:00:00 - 2026-01-21 11:00:00
  Data Points: 60
  
  Statistics:
    Min:    5.2%
    Max:    45.8%
    Mean:   22.1%
    Median: 18.9%
    Sum:    1,326%
  
  Raw Data (first 5 points):
    10:01:00 - 5.2%
    10:02:00 - 6.1%
    10:03:00 - 7.3%
    10:04:00 - 8.9%
    10:05:00 - 10.2%
```

### Validation Rules

| Field | Rule | Rationale |
|-------|------|-----------|
| `metric_name` | Non-empty | Must identify the metric |
| `unit` | Non-empty | Users need to understand values |
| `datapoints` | Sorted by timestamp | Consistent ordering |
| `datapoint_count` | Matches `len(datapoints)` | Data integrity check |

---

## Entity: QueryError

**Purpose**: Represent errors in query execution or LLM parsing.

**Responsibility**: Provide actionable error information to users.

**Persistence**: Stateless - used in single response, not stored.

### Fields

```python
class QueryError(BaseModel):
    """
    Represents an error in query execution.
    Provides actionable information to help users correct their query.
    """
    
    # Error classification
    error_type: Literal[
        "parsing_error",      # LLM couldn't interpret query
        "metric_not_found",   # Requested metric doesn't exist
        "invalid_time_range", # Time parameters invalid
        "grafana_unavailable", # Can't reach Grafana
        "unsupported_operation", # Query asks for unsupported feature
        "invalid_query"       # Query parameters rejected by Grafana
    ] = Field(
        description="Category of error"
    )
    
    # Error details
    message: str = Field(
        description="Human-readable error message"
    )
    
    # Recovery guidance
    suggestion: Optional[str] = Field(
        default=None,
        description="Suggestion for how to fix or rephrase the query"
    )
    
    # Debug info (for logs, not user display)
    details: Optional[str] = Field(
        default=None,
        description="Detailed error information for logging"
    )

class Config:
    json_schema_extra = {
        "examples": [
            {
                "error_type": "metric_not_found",
                "message": "Metric 'disk_io' not found in Grafana",
                "suggestion": "Try 'disk_write_rate', 'disk_read_rate', or 'disk_usage'"
            },
            {
                "error_type": "unsupported_operation",
                "message": "Anomaly detection is not supported",
                "suggestion": "Try asking for the raw metric data instead, e.g., 'Show CPU usage for the last hour'"
            },
            {
                "error_type": "parsing_error",
                "message": "Could not understand your query",
                "suggestion": "Try phrasing it as: 'Show [metric_name] for the [time_period]' (e.g., 'Show CPU usage for the last hour')"
            }
        ]
    }
```

### Error Handling Rules

| Error Type | Message | Suggestion |
|------------|---------|-----------|
| `parsing_error` | "Could not understand your query" | Show example formats |
| `metric_not_found` | "Metric '[name]' not found" | List available metrics |
| `invalid_time_range` | "Time range is invalid" | Explain valid format |
| `grafana_unavailable` | "Cannot reach metrics system" | Check if Grafana is running |
| `unsupported_operation` | "Feature is not supported" | Explain what is supported |

---

## State Model

The agent state combines all entities into a unified query state:

```python
from typing_extensions import TypedDict

class MetricsQueryState(TypedDict):
    """
    Complete state for a metrics query in the agent workflow.
    """
    
    # Input
    user_query: str
    
    # Parsed query
    parsed_query: Optional[MetricsQuery]
    
    # Result
    result: Optional[MetricsQueryResult]
    
    # Error (if any)
    error: Optional[QueryError]
    
    # Metadata
    processing_time_ms: float  # For monitoring
```

---

## Relationships

```
User Input (string)
    │
    ▼
MetricsQuery (validated parameters)
    │
    ├─ Valid: Query Grafana
    │   │
    │   ▼
    │ MetricsQueryResult (time-series data)
    │
    └─ Invalid: Error
        │
        ▼
    QueryError (actionable error)

MetricsQueryState ties everything together in the agent workflow.
```

---

## Example: Complete Query Cycle

### Input
```
User: "Show me CPU usage for the last hour"
```

### Processing

**Step 1: Parse to MetricsQuery**
```python
query = MetricsQuery(
    metric_name="cpu_usage",
    time_range=TimeRange(
        start_time=datetime.now() - timedelta(hours=1),
        end_time=datetime.now()
    ),
    aggregation=None,
    confidence=0.95
)
```

**Step 2: Query Grafana → MetricsQueryResult**
```python
result = MetricsQueryResult(
    metric_name="cpu_usage",
    unit="%",
    time_range=query.time_range,
    datapoints=[
        DataPoint(timestamp=..., value=12.5),
        DataPoint(timestamp=..., value=13.2),
        # ... 60 datapoints total
    ],
    datapoint_count=60,
    statistics=AggregationStats(
        min=10.2,
        max=45.8,
        mean=22.1,
        ...
    )
)
```

**Step 3: Format for Display**
```
CPU Usage
  Time: 2026-01-21 10:00:00 to 2026-01-21 11:00:00
  Data Points: 60
  
  Min:  10.2%
  Max:  45.8%
  Mean: 22.1%
  
  [Full time series shown in agent response]
```

---

## Notes

- **No Custom Parsing**: LLM generates properly formatted JSON matching MetricsQuery schema
- **Validation on Creation**: Pydantic validates all fields when models instantiated
- **Human-Readable Output**: `summary` property and formatting methods generate user-facing text
- **Error Recovery**: QueryError provides specific guidance for fixing queries
- **Extensibility**: Additional fields can be added to entities without breaking existing queries
